\chapter{Feature Selection} \label{chap:selection}

Recall that each feature tells
us something about the training example. Ideally, it should also help us
classify the example. However, in practice and particularly
in medicine, a large number of attributes about a patient is usually recorded,
not all of which are relevant in predicting the class.
Having such a large number of features
is detrimental for two reasons: the training time is significantly
increased because the classifier must incorporate irrelevant information, and
this irrelevant information tends to degrade the performance
of even state-of-the-art classifiers such as C4.5 decision trees
\cite{Witten2005}. Clearly, there is an advantage in considering only a subset
of the most relevant features; to find them, we will use feature selection.

\section{Feature Selection}
After data preparation, selecting a subset of features or attributes to use
in a model is also important \citep{Kotsiantis2006},
and helps to reduce noise and redundant
attributes from becoming part of the final model. As we mentioned earlier,
experiments have found that
the performance of various classification methods such as decision trees
\textit{deteriorates} in the presence of useless attributes.
Our data set contains 73
non-target attributes, which is a fairly high number. Many of these attributes
are derived from others in the set: for example, whether or not a patient is
over 65 years of age (the \textit{age65} feature) is directly dependent on the
value of their \textit{age} attribute.

We began the process of feature selection by removing attributes that were
not available upon admission, as these tended to be highly related to the
LOS48 outcome and were a consequence, not a predictor, of LOS. These attributes
were: iculos, outcome, disp from ED, disp from hosp, died, rehab and los.
The id attribute was also removed because it simply numbered each record and
indicated nothing about the patient themself. This left us with 67 attributes.

The work of actually selecting the attributes out of the 67 available was
carried out in two ways: automated using feature selection methods, and
manually using advice from a domain expert.

\subsection{Automatic Methods}
There are a number of methods that are commonly used to automate the selection
of a subset of features for the final classifier. The methods we used were:
\begin{itemize}
  \item \textit{Correlation-based feature selection} \citep{Hall2000},
  which evaluates the `merit' of a
  subset of features by considering their degree of correlation with the target
  variable compared to their degree of correlation amongst each other. High
  correlation between variables indicates redundancy. Under
  this scheme, a set of features that are highly correlated with the target
  while having low correlation with other features in the set is ideal.
  \item Selection based on the
  \textit{Pearson product-moment correlation coefficient}, a measure of the
  linear correlation or dependence between two variables.\todo{add some more info}
  \item \textit{Information gain} \todo{explain a bit about this}
  \item Feature selection based on the \textit{OneR classifier}
  \citep{Holte1993}, which uses one
  attribute as the predictor for the target. This attribute is typically the one
  that has the lowest prediction error. Feature selection using this scheme
  involves first ranking attributes based on their error, with the lowest error
  attributes as the most desirable. We then selected the top 20 attributes to
  use in the building of the model.
  \item Wrapper feature evaluation \todo{explain}
\end{itemize}

We used the WEKA 3.7.11 \citep{Hall2009} implementation of the above
methods, after converting the data into the Attribute-Relation File Format
(ARFF)\footnote{\url{http://weka.wikispaces.com/ARFF+\%28developer+version\%29}}
most suitable for the program.
\todo{give more details, ie specifying binary/numeric
attributes, maybe specify in appendix?}

\subsection{Manual Selection}
We also wanted to compare the feature selection of an expert in the domain
of the data set, namely trauma, with the models obtained from the features
selected using the automated methods described above. This resulted in two
feature subsets. The first was obtained simply by presenting the entire list
of 73 features to the domain expert and asking him to select the features he
thought were most indicative of the target variable from experience. The
second set was obtained from the work of Dinh et. al. \citep{Dinh2013a} that
we are building on, giving us a baseline to compare our results to.

Altogether, the various feature selection methods resulted in a total of
7 data sets (4 automated, 2 expert selected, and 1 with all features) which
we used in the model construction phase.
