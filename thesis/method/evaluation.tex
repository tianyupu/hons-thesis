\chapter{Evaluation} \label{chap:evaluation}

\section{Data Sets}
\subsection{Collection}
The data set used consisted of trauma registry data from the trauma centre
at the Royal Prince Alfred Hospital, a major trauma centre in New South Wales,
Australia. It covered all adult (age 15 and over) inpatient admissions to the
trauma centre from 2007--2011. 

All patients were first admitted to the trauma ward until discharged
or transferred to an appropriate unit within the hospital. A single trained
data manager recorded a variety of attributes about the admitted patient,
such as age, gender, blood pressure, mechanism of injury and body regions
that were injured. We received this data in a Microsoft Excel spreadsheet.

\subsection{Characteristics}
There were 2546 patient records in the data set we received, comprising of 74
features, one of which was the target variable \textbf{LOS48}. LOS48 was a
binary variable -- that is, it could only take two values, 0 or 1 -- with 1
indicating that the patient stayed two days or less, and 0 indicating a stay
of greater than 2 days.

\todo{Perhaps insert some more information about the data itself, such
as the various attributes and their data types? Not sure if that should go
here, in the results, or in an appendix -- it is relevant but can disturb
the flow of the main text.}

\section{Performance evaluation} % don't forget cross-validation
\label{sec:perfeval}
Here we describe the methods used in order to assess the performance of the
models constructed through single classifiers and ensemble methods described
above.

\subsection{Single classifier}
First we sought to compare the suitability and performance of various
classifiers by themselves (later we looked at \textit{ensembles} of
classifiers, when more than one is involved in making a prediction).
These are the
classifiers studied: \todo{expand a bit more here, maybe list the
configuration parameters}
\begin{itemize}
  \item Na\"{i}ve Bayes
  \item Decision tree (C4.5)
  \item Logistic regression
  \item Support vector machine
  \item $K$-nearest neighbour
  \item Feed-forward neural network (multilayer perceptron)
\end{itemize}

Each classifier was trained using ten-fold cross-validation for each of the
7 data sets obtained from the various feature selection methods described
above. This was repeated 30 times for each classifier and the reasons for
this, as well as the metrics used to assess the performance of these
classifiers, are described in Section \ref{sec:perfeval} of this chapter.

In addition to the feature selection methods, we also used the WEKA 3.7.11
\citep{Hall2009} implementation of the above classifiers for our experiments.
Training was automated using a Bash shell script (whose source can be found
in the appendix) that programmatically read in a list of classifiers to
train and repeated the procedure any specified number of times, writing the
results to a specified directory for later reference.

\subsubsection{Parameter tuning}
It is difficult in data mining to apply an algorithm out-of-the-box and have
it perform optimally on a given data set \citep{Witten2005}. Therefore, time
must be spent tweaking and tuning the parameters of a classifier in order to
improve classification performance. This was the next step we took. We
considered a range of values for various parameters of each classification
scheme, and tried some of these new configurations by hand and some by using
WEKA's cross-validation parameter selection class. The configurations tested,
as well as how they were executed, are listed in this table:
\todo{insert table!}

\subsection{Ensemble classifiers}
In order to improve upon the performance of individual classifiers, we also
tested the performance of ensembles of classifiers -- that is, combining the
predictions of several different classifiers, or several of the same
classifier. Specifically, the ensemble methods used were:
\begin{itemize}
  \item Random Forests
  \item Bagging
  \item Stacked generalisation (stacking)
  \item Boosting (AdaBoostM1)
\end{itemize}

Again, we were able to create predictive models using the implementations of
these ensemble methods in WEKA 3.7.11. Since ensemble classifiers are not
actual classifiers but a way to train and combine the predictive power of more
than one classifier, we list the combinations of classifiers used in our
ensemble methods:
\todo{insert table!}

\subsection{Metrics}
There are many available and commonly used metrics that we can use when
evaluating and comparing the performance of prediction models. The ones that
we use are: \todo{maybe put these in a table}
\begin{itemize}
  \item Accuracy
  \item Specificity
  \item Sensitivity
  \item Area under the receiver-operating characteristic (ROC) curve, or AUC
\end{itemize}

Since we only have one data set, we use ten-fold stratified cross-validation
to obtain values for the above metrics. We run the ten-fold cross-validation
40 times for each classifier (single or ensemble) for each of the 7 data sets
produced from feature selection. 40 runs was necessary in order for us to
compute confidence intervals for each of the metrics, which makes use of the
Central Limit Theorem from statistics.

\subsubsection{Confidence intervals}
In order to compare our results with that of Dinh et. al. \citep{Dinh2013a},
we are especially interested in the mean cross-validated AUC of our
classifiers as well as
the upper and lower bounds of the 95\% confidence interval associated with this
mean. The intervals are constructed using this argument:

For each classifier, let the population of all of its ten-fold cross-validated
AUC scores be some random variable $X$, which follows a distribution whose
parameters (such as mean and standard deviation) we do not know.
The Central Limit Theorem states that given a random
sample of $n$ items from $X$, namely $X_1,X_2,\dots,X_n$, the sample mean
$\bar{x}$
is approximately normally distributed when $n$ is at least roughly around 30
and the $X_i$'s are independent and identically distributed.
Consequently, we can construct a $(1-\alpha)$\% confidence interval for
$\bar{x}$ -- where $\alpha$ is commonly called the
\textit{significance level} -- as follows:
\begin{equation}
  \left[\bar{x} - z_{1-\alpha/2}\dfrac{s}{\sqrt{n}},
    \bar{x} + z_{1-\alpha/2}\dfrac{s}{\sqrt{n}}\right]
\end{equation}
where $\bar{x}$ is the sample mean, $\alpha$ is the significance level
(5\% or $0.05$ in our case), $s$ is the sample standard deviation, $n$
is the number of observations in our sample, and $z_{1-\alpha/2}$ is the value
of the standard normal variable $Z \sim \mathcal{N}(0,1)$ with cumulative
probability $1-\alpha/2$. Since we have taken 40 independent samples of the
ten-fold cross-validated AUC, the Central Limit Theorem holds and we can
compute the mean and standard deviation of our 40 samples in order to
construct a 95\% confidence interval for the mean AUC of each classifier.

\subsection{Statistical tests}
To decide whether or not the difference in values of each metric between
classifiers was statistically significant, paired $t$-tests were conducted for
a subset of the results: \todo{elaborate}
\begin{itemize}
  \item Some feature set vs other feature set
  \item Single classifier vs other single classifier
  \item Ensemble vs single
  \item Our best method vs Dinh2013's method
\end{itemize}
