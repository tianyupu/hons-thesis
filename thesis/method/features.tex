\chapter{Feature Preprocessing and Selection} \label{chap:features}

In the previous chapter we began by describing each training example as a
vector of feature values, and then described how a range of different
learning algorithms used those feature vectors to learn a way to
distinguish examples of one class from another. Ideally, all features given
in a data set should be relevant and useful in not only describing the training
example but also predicting the class.
In practice, and particularly
in medicine, a large number of attributes about a patient is usually recorded.
This is detrimental for two reasons: the training time is significantly
increased because the classifier must incorporate irrelevant information, and
this irrelevant information tends to degrade the performance
of even state-of-the-art classifiers such as C4.5 decision trees
\cite{Witten2005}. Clearly, there is an advantage in considering only a subset
of the most relevant features; to find them, we will use feature selection.

It is also common for data to require preprocessing before a classifier can
be trained upon it -- indeed, even before feature selection is performed.
Otherwise, it will be hard to know if the results we see are reflective of
the algorithms we use or simply a case of lucky data. Preprocessing involves
cleaning and transforming the data, and is critical to the performance of
learning algorithms that will be used to learn from it.

In this chapter we describe the various preprocessing steps performed on
the data, as well as the feature selection methods we evaluate in our work.

\section{Preprocessing}
In any data mining task, adequately preparing the data ensures the best
chance of success: this includes deciding how to deal with missing and
inaccurate values, as well as appropriately transforming the data
\citep{Witten2005} and selecting or constructing the most suitable features
for the task \citep{Kotsiantis2006}.
In this section we describe how we addressed each of
the major issues and the reasons for our choice.

\subsection{Cleaning} % missing values, irrelevant features
The exact steps involved in \textit{cleaning} the data will vary between data
sets, but the overall process is similar. Common issues to deal with include:
\begin{itemize}
\item Incorrect spelling or punctuation, perhaps as a result of mistakes in
data entry
\item Leading or trailing whitespace, and other garbage characters,
perhaps as a result of data entry or
the system that stores the data
\item Missing values for various features or for the class label
\item Values for features that do not make sense (such as a patient age of
-1)
\end{itemize}

Since the data was initially in an Excel spreadsheet, we explored the data
by hand using Microsoft Excel's filter function on each column and
consolidated many categorical values simply
by correcting spelling and punctuation. For example:
\begin{itemize}
  \item In the \textbf{sex} attribute, there were values of `Female' and
  `Femal' which were consolidated into `Female'.
  \item In the \textbf{mechanism} attribute, which was textual and indicated
  the category of how the patient was injured, there were such categories as
  `Other Vehi' and `Other Vehicle' as well as `Pedal Cycl' and `Pedal Cyclist'.
  Such categories were combined into `Other Vehicle' and `Pedal Cyclist'
  respectively.
  \item Some categories in \textbf{mechanism} were also divided into whether or
  not the mechanism of injury was self-inflicted, such as `Shooting' and
  `Shooting-selfinflicted'. When we asked the domain expert, Michael Dinh, his
  suggestion was to combine them into one category: in this case, simply
  `Shooting'; this was done for all the categories in this attribute.
  \item \textbf{disp from ED} indicated the section of the hospital that the
  patient was discharged to. This was also a textual categorical attribute.
  There were redundancies like `G. ICU' and `G.ICU' and even `GICU', which we
  all combined into one category, `GICU'.
\end{itemize}

The data was then exported into CSV (comma-separated value) format, so that
we could continue to process it programmatically rather than manually. Many
fields in the data set contained leading or trailing whitespace, which was
removed with a few lines of Python code (see appendix \todo{[add reference
to appendix]}). All attribute names were converted to lowercase. In the case
where attribute names consisted of several words, the spaces were replaced
with underscores (\_) to ensure that the full attribute name was read.
For example, ``Any Cancer'' was converted into ``any\_cancer''.

In order for our results to be comparable to that of Dinh
et. al. \citep{Dinh2013a}, we removed all instances with missing values,
leaving 2517 instances with which to use to create our prediction models.

\subsection{Normalisation}
All numeric attributes were normalised into the range $[-1,+1]$ 

\subsection{Discretisation}

\section{Feature Selection}
After data preparation, selecting a subset of features or attributes to use
in a model is also important, and helps to reduce noise and redundant
attributes from becoming part of the final model. As we mentioned earlier,
experiments have found that
the performance of various classification methods such as decision trees
\textit{deteriorates} in the presence of useless attributes.
Our data set contains 73
non-target attributes, which is a fairly high number. Many of these attributes
are derived from others in the set: for example, whether or not a patient is
over 65 years of age (the \textit{age65} feature) is directly dependent on the
value of their \textit{age} attribute.

We began the process of feature selection by removing attributes that were
not available upon admission, as these tended to be highly related to the
LOS48 outcome and were a consequence, not a predictor, of LOS. These attributes
were: iculos, outcome, disp from ED, disp from hosp, died, rehab and los.
The id attribute was also removed because it simply numbered each record and
indicated nothing about the patient themself. This left us with 67 attributes.

The work of actually selecting the attributes out of the 67 available was
carried out in two ways: automated using feature selection methods, and
manually using advice from a domain expert.

\subsection{Automatic Methods}
There are a number of methods that are commonly used to automate the selection
of a subset of features for the final classifier. The methods we used were:
\begin{itemize}
  \item \textit{Correlation-based feature selection} \citep{Hall2000},
  which evaluates the `merit' of a
  subset of features by considering their degree of correlation with the target
  variable compared to their degree of correlation amongst each other. High
  correlation between variables indicates redundancy. Under
  this scheme, a set of features that are highly correlated with the target
  while having low correlation with other features in the set is ideal.
  \item Selection based on the
  \textit{Pearson product-moment correlation coefficient}, a measure of the
  linear correlation or dependence between two variables.\todo{add some more info}
  \item \textit{Information gain} \todo{explain a bit about this}
  \item Feature selection based on the \textit{OneR classifier}
  \citep{Holte1993}, which uses one
  attribute as the predictor for the target. This attribute is typically the one
  that has the lowest prediction error. Feature selection using this scheme
  involves first ranking attributes based on their error, with the lowest error
  attributes as the most desirable. We then selected the top 20 attributes to
  use in the building of the model.
  \item Wrapper feature evaluation \todo{explain}
\end{itemize}

We used the WEKA 3.7.11 \citep{Hall2009} implementation of the above
methods, after converting the data into the Attribute-Relation File Format
(ARFF)\footnote{\url{http://weka.wikispaces.com/ARFF+\%28developer+version\%29}}
most suitable for the program.
\todo{give more details, ie specifying binary/numeric
attributes, maybe specify in appendix?}

\subsection{Manual Selection}
We also wanted to compare the feature selection of an expert in the domain
of the data set, namely trauma, with the models obtained from the features
selected using the automated methods described above. This resulted in two
feature subsets. The first was obtained simply by presenting the entire list
of 73 features to the domain expert and asking him to select the features he
thought were most indicative of the target variable from experience. The
second set was obtained from the work of Dinh et. al. \citep{Dinh2013a} that
we are building on, giving us a baseline to compare our results to.

Altogether, the various feature selection methods resulted in a total of
7 data sets (4 automated, 2 expert selected, and 1 with all features) which
we used in the model construction phase.

