\chapter{LOS Classification Using Supervised Learning}
  \label{chap:classification}

In order to use a supervised learning algorithm to identify objects with
a particular classification, we first need \textit{feature vectors} to
describe these objects. In our case, each patient is converted into one
feature vector, where each \textit{feature} in this vector tells us
something about the
patient that can help us decide which LOS category he or she should be
placed in. The features comprising each feature vector must be the same,
but with different values. By comparing feature values between different
vectors, we are able to decide which \textit{class} or category a patient
belongs in. For example, if we wanted to classify the weather on a given
day as ``good'' or ``bad'', we could use two features -- temperature and
the amount of rain -- which are likely to be key indicators. Classifying
several days simply involves creating a feature vector that corresponds
to each day, with differing values for the temperature and the amount of
rain, and using these values to decide whether or not a day had good or
bad weather.

Since we will be using supervised learning techniques, we need
\textit{training examples}, which consist of a set of feature vectors
that are already labelled with the class that it belongs in. These
training examples are then used by a classifier to ``learn'' the
relationship linking the feature vectors to their class labels. Once we have
``trained'' the classifier to learn this relationship with the pre-labelled
data, it will then be able to predict the class of any feature vector that we
give it.

In this
chapter we describe each of the learning algorithms we used to model the
relationship between the feature vectors and the LOS.

\section{0R and 1R}
Short for ``0-rule'' and ``1-rule'' respectively, 0R and 1R are
very simple classifiers. 0R uses no features to predict the class, and
1R only uses one.

Given a data set, the 0R classification algorithm will find the class that
occurs the most often, and predict this class for all new examples. For LOS
classification, if one class in a data set was ``LOS $\leq$ 2 days'' and
the other was
``LOS $>$ 2 days'', and feature vectors (which represent patients in this
scenario) were more commonly labelled ``LOS $\leq$ 2 days'', then 0R will
predict all unseen patients with ``LOS $\leq$ 2 days''.
The prediction decision of 0R is very easy to understand,
and its results also provide an apt baseline for evaluating the more
sophisticated classifiers used in this work.

1R works by considering each feature individually.
For each of the values of the
feature, 1R finds the class with the highest frequency and creates a rule
from this observation. If the feature is numeric (that is, its values are
continuous rather than discrete), then 1R will partition the spectrum of
values into discrete buckets. This is known as \textit{discretisation}.
The error rate, expressed as the ratio of correct predictions to the total
number of examples in the data set,
is calculated for each rule that is generated
in this way. The rule with the lowest error rate is then chosen as the 1-rule
for this classifier,
and if there is a tie, it is broken randomly. As with 0R, the prediction
decision of 1R is transparent and it provides another baseline upon which
more sophisticated classifiers can be evaluated. Studies have also found that
1R performs quite well in terms of accuracy relative to other classifiers
\cite{Holte1993}.

Concretely, given a feature
vector of the form \textit{(age, gender, has\_cancer)},
1R will split the ages of
all patients into discrete categories
(such as \textit{0-20, 21-40, 41-65, 66+}). It
will then check the class labels for all training examples in the data set
that have \textit{age} in the 0-20 category
(as that is the first value for the discretised \textit{age} feature) and
note the most common class, proceeding similarly for the rest of the
categories of \textit{age}. Then it will consider \textit{gender}, which has
two possible values $\{$\textit{male, female}$\}$, and finally \textit{has\_cancer}
with two values $\{$\textit{true, false}$\}$. This results in 8 rules,
corresponding to three features.
An example of a rule is that \textit{age 0-20} most commonly
has ``LOS $\leq$ 2 days''. For each of these rules, 1R will attempt to
use only that rule to predict the LOS class for each example in the data set,
noting down the error rate. 1R then chooses the feature that produces rules
with the lowest total error rate.

\section{Na\"{i}ve Bayes (NB)}
Unlike 1R, which considers only one feature in the feature vector, Na\"{i}ve
Bayes (NB) classifiers allows all features to contribute equally to the
classification decision. Such classifiers apply Bayes' theorem with the
(unrealistic) assumption of independence between features, hence the
description \textit{na\"{i}ve}. This means that we
calculate the probability that a particular feature value belongs in a class
without considering the effect of other features on this probability. This
probability can be computed by
counting the frequency with which the value is associated with a class in the
training data. After these probabilities are computed, classifying a feature
vector simply involves multiplying the probabilities of each feature belonging
to each possible class. The predicted class for that feature vector is then the
class with the highest probability.

Speaking mathematically, Bayes' theorem states that:
\begin{equation*}
\mathbb{P}(Class_i|F_1,F_2,\ldots,F_n) =
  \dfrac{\mathbb{P}(F_1,F_2,\ldots,F_n|Class_i)\mathbb{P}(Class_i)}{\mathbb{P}(F_1,F_2,\ldots,F_n)}
\end{equation*}
Since we assume that the features are independent, we can simplify the above as:
\begin{equation*}
\mathbb{P}(Class_i|F_1,F_2,\ldots,F_n) =
\dfrac{\mathbb{P}(F_1|Class_i)\mathbb{P}(F_2|Class_i)\ldots \mathbb{P}(F_n|Class_i)\mathbb{P}(Class_i)}{\mathbb{P}(F_1,F_2,\ldots,F_n)}
\end{equation*}
This gives us the probability that the feature vector $(F_1,F_2,\ldots,F_n)$
belongs in the $i$-th class. Classifying the feature vector involves
calculating this probability for each for each class and choosing the class
with the highest probability.

Since we compute the probability of each feature value belonging in a
particular class by counting the frequency of each value's occurrence with
respect to the class, it is sometimes possible that a feature value has zero
probability of being in a certain class. This would make the numerator of the
above equation zero. A feature vector that could otherwise have been highly
likely to belong in class $i$ is now counted as having zero probability of
belonging in the class. In order to ensure that each probability is non-zero,
a small constant is added to all probabilities in a process called
\textit{Laplace smoothing}, which is commonly used for NB classifiers.

Like 1R, numeric features also need to be handled separately from nominal ones.
The typical approach is to assume that the values of a numeric feature follow a
normal (or Gaussian) distribution with mean and standard deviation calculated
from the values that the feature takes with respect to each class.
This allows us to use the probability
density function of the normal distribution to calculate the probabilty of the
feature belonging to a particular class:
\begin{equation*}
\mathbb{P}(F_i=x_i|Class_i) =
  \dfrac{1}{\sqrt{2\pi}\sigma}e^\frac{(x_i-\mu)^2}{2\sigma^2}
\end{equation*}

Despite the na\"{i}ve independence assumption of the NB classifier, it often
works surprisingly well in practice and is relatively fast to train. We use it
in our work both as a simple classifier in its own right, and also as a more
sophisticated baseline to 0R and 1R.

\section{C4.5 Decision Tree (DT)}
Decision trees (DT) can be specified recursively and are an example of a
``divide-and-conquer'' approach to learning. First, we choose a feature to
place at the root node and then create one branch for each possible value.
This partitions the data set into subsets, one for every value that the
feature takes. We repeat this for each branch, choosing different features
each time and continuing to divide the data set into progressively smaller
subsets, growing the tree.
If all examples in a subset have the same class at any point during
this process, we stop splitting that subset.

The only thing left to specify
now is how to decide which attribute to split on. Note that when splitting,
we would like to create subsets that are as ``pure''
-- that is, containing as many
instances of the same class -- as possible, so that the splitting may
terminate earlier. Since we would like our trees to be as small as possible to
prevent overfitting and allow the best generalisation ability,
we would therefore like to choose the feature to split on such that the
created subsets were as pure as possible. The measure of purity used in
learning decision trees is the \textit{information}, and is measured in
\textit{bits}. At any node (or subset) of the tree, this measure represents
the expected amount of information that would be needed to classify a new
example, given that the example reached the node. The information at each
node is calculated using the \textit{entropy} function:
\begin{equation*}
\mathrm{entropy}(p_1,p_2,\ldots,p_n) = -p_1\mathrm{log}p_1 -p_2\mathrm{log}p_2 \ldots -p_n\mathrm{log}p_n
\end{equation*}
Given $k$ classes, $c_1,\ldots,c_k$ is the number of instances in each
class at a node, and $\sum_{i=1}^k c_i = c$ is the total number of instances
at that node, the information of that node is then:
\begin{equation*}
\mathrm{info}(c_1,\ldots,c_k) = \mathrm{entropy}\left(\frac{c_1}{c},\frac{c_2}{c},\ldots,\frac{c_k}{c}\right)
\end{equation*}

To find the attribute to split on, we assume we are splitting on a particular
attribute, and calculate the information at the node given the split. We do
this for each attribute, and compute the \textit{information gain} from
splitting on that attribute:
\begin{equation*}
\mathrm{gain}(feature) = \mathrm{info}(\text{child node}) - \mathrm{info}(\text{parent node})
\end{equation*}
The feature that is chosen is therefore the one with the highest information
gain.

Decision trees are fast to train and produce models that are straightforward
for humans to interpret, the latter of which is particularly valuable in a
clinical environment. We will use the C4.5 tree induction algorithm, which is
considered the state-of-the-art and handles different types of feature values
as well as missing values.

\section{Logistic Regression (LR)}
Logistic regression is a regression technique used for when the dependent
variable is dichotomous or two-valued, and thus it is a suitable candidate to
use for our LOS classification problem.
Previous work on trauma LOS prediction by
Dinh et. al. \cite{Dinh2013a}, as well as many attempts at modelling the LOS
in various medical domains, have all used logistic regression, often as a
starting point.

For a class variable with two possible values that occur with
probability $p$ and $1-p$ respectively, and a feature vector
$(F_1,F_2,\ldots,F_n)$, the logistic regression model is:
\begin{equation*}
\begin{aligned}
\mathrm{logit}(p) &= \mathrm{log} \left(\dfrac{p}{1-p}\right) \\
  &= \beta_0 + \beta_1F_1 + \beta_2F_2 + \ldots + \beta_nF_n
\end{aligned}
\end{equation*}
For LOS classification, this can be rearranged to:
\begin{equation*}
\begin{aligned}
p &= \mathbb{P}(Class=\mathrm{LOS }\leq2\mathrm{ days}|F_1,F_2,\ldots,F_n) \\
  &= \dfrac{e^{\beta_0 + \sum^n_{i=1}\beta_i F_i}}{1 + e^{\beta_0 + \sum^n_{i=1}\beta_i F_i}}
\end{aligned}
\end{equation*}

Training the logistic regression classifier, then, is a matter of finding the
$\beta_i$s that will maximise the likelihood (conditional probability of the
data given the values for $\beta_i$) of the given data set. This is done using
numerical methods, where estimates of the $\beta_i$s are chosen, the likelihood
computed, and the $\beta_i$ are updated, and so on: this process is repeated
until parameter estimates converge or do not change more than a given threshold
value.

To classify a feature vector $x = (x_1,x_2,\ldots,x_n)$, we substitute the
$x_i$'s into the above equation in place of the $F_i$'s, which gives us a
probability. If this probability is above a particular threshold, then we
classify the example as belonging to the class ``LOS $\leq$ 2 days'', otherwise
it is classified as ``LOS $>$ 2 days''. Usually this threshold is 0.5.

\section{Support Vector Machine (SVM)}
Support vector machines (SVMs) are an example of a supervised learning algorithm
capable of classifying training examples into two classes. They are one of the
best algorithms in terms of predictive accuracy \cite{Bellazzi2008}. Despite
this, they have not been widely applied to predicting the LOS, which gives us
an opportunity to evaluate their performance in our LOS classification problem.

Given a set of $n$-feature vectors, we can represent each vector as a point in
$n$-dimensional space, where each feature is a dimension. The SVM will try to
find a \textit{hyperplane} that separates the two classes. Usually there are
many such hyperplanes; the distinguishing aspect of an SVM is that it will
find the hyperplane that maximises the distance between the two nearest points
in each class. This distance is called the \textit{margin}, and the intuition
is that maximising it will give us the greatest possible separation between the
classes -- the classifier comes no closer to either class than it needs to.
This gives the SVM the best generalising ability to classify new examples.

Figure \ref{fig:svm} shows an example of a two-dimensional feature space, with
two classes of points (black and white). We can see the hyperplane, in this
case a line since we are only in two dimensions, separating the two classes
with as much margin as possible. In
this case, the points are linearly separable: all white points lie on one side
of the hyperplane, and all black points lie on the other. In practice, this is
not common, and often data cannot be separated cleanly by a hyperplane.
Note that the points closest to the margin are those that are most likely to be
ambiguous when the data cannot be cleanly separated. The SVM will progressively
remove more and more of these ambiguous points from consideration when it
cannot find a hyperplane until it finds a suitable one.

\begin{figure}[h]
\label{fig:svm}
\caption{}
\centering
%\includegraphics[scale=0.95]{images/a2-nn}
\end{figure}

Instead of removing points from consideration by the SVM, we can also specify
a \textit{kernel function}, which transforms the original feature space into
some other non-linear space. Commonly, polynomial functions of degree 2 or
greater are used, and theoretically they can approximate non-linear boundaries
between classes to any arbitrary accuracy. However, this is usually not
practically possible with data sets containing even a modest number of features
(around 10), due to the computational complexity of estimating a large number
of coefficients that are introduced by the transformation.

SVMs can be specified mathematically as a type of optimisation problem known
as \textit{constrained quadratic optimisation}. Given $m$ training vectors
$\mathbf{x_i} \in \mathbb{R}^n, i=1,\ldots,m$ and their labels
$y_i \in \{-1,+1\}, i=1,\ldots,m$, the optimisation problem can be written as:
\begin{equation}
\label{eqn:svm1}
\begin{aligned}
& \underset{\mathbf{w},b}{\mathrm{minimise}}
  && ||\mathbf{w}|| + C\sum_{i=1,\ldots,m} s_i \\
& \text{subject to}
  && y_i(\mathbf{w}^T\phi(\mathbf{x_i})+b) \geq 1 \\
& && s_i \geq 0, i = 1,\ldots,m
\end{aligned}
\end{equation}
where $\phi(\mathbf{x})$ is the kernel function, and
$\mathbf{w}\cdot\mathbf{x_i} + b = 0$ is the hyperplane that the SVM finds.
The $s_i$'s allow the SVM to ignore some examples if it cannot find a suitable
hyperplane, and $C$ (sometimes called the SVM's complexity constant) is a
constant specified by the user that controls how much we want to balance
finding the best possible margin with allowing misclassified examples.
Note that for a linear kernel, $\phi(\mathbf{x}) = \mathbf{x}$. Training the
SVM involves solving this optimisation problem and obtaining values for
$\mathbf{w}$ and $b$.

To classify an instance $\mathbf{x_{new}}$, we simply find the sign of
$\mathbf{w}\cdot\mathbf{x_{new}}+b$. This will tell us which one of the
two classes the new example belongs to.

\section{$k$-Nearest Neighbour ($k$-NN)}
In this section we will describe the $k$-nearest neighbour ($k$-NN) algorithm
and discuss some extensions which we use in our work. Additionally, we will
describe our contribution, which is an extension of the distance metric for
finding the nearest neighbours.

\subsection{Basic Algorithm}
The $k$-nearest neighbour classifier is an example of an
\textit{instance-based} learning algorithm. This is because it does not, unlike
the other classifiers described in this section, attempt to deduce or
generalise a relationship between the features and the class: it simply stores
all training examples and classifies an unseen example by finding the $k$
``nearest'' examples (or \textit{instances}) and assigning the majority class
to the new example. In the simplest form of the algorithm, $k$ is chosen to be
1 and the Euclidean distance is used to measure the closeness of neighbours:
\begin{equation*}
\mathrm{Distance} = \sqrt{(x_1-y_1)^2 + (x_2-y_2)^2 + \ldots + (x_n-y_n)^2}
\end{equation*}
where $x_i,y_i$ are the values of the $i$-th feature for two different feature
vectors (or instances).

Like the Na\"{i}ve Bayes classifier, $k$-NN allows all features to be taken
into account equally when classifying a new example. However, since feature
values often span different ranges and units of measurement, a particularly
large range of values for a feature would contribute much more to the distance
than another feature that spans a lower set of values.
To avoid this, it is important to \textit{normalise} the
values of each numeric feature before using $k$-NN: for each feature $i$, we
calculate the normalised value of the feature for each training example. This
is done using the following relationship:
\begin{equation*}
a_i = \dfrac{v_i - \mathrm{min }v_i}{\mathrm{max }v_i - \mathrm{min }v_i}
\end{equation*}
where $a_i$ is the normalised value, $v_i$ is the actual value in the data
set, and max $v_i$ and min $v_i$ are taken over all training examples.

\subsection{Extensions}
There have been many extensions proposed to the $k$-NN algorithm described
above. These usually seek to find a more suitable distance metric for the
problem domain at hand, or to reduce the storage requirements of the original
$k$-NN algorithm. We will focus on two extensions to the Euclidean distance:
K* and our contribution, Ranked Distance.

\subsubsection{K*: Instance-based Classifier with Entropic Distance Function}

\subsubsection{Ranked Distance}
\paragraph{Intuition}
As we mentioned earlier, all features are considered equally in a $k$-NN
classifier, which implies that all features are equally important in predicting
the class. In practice, however, this is not always the case: to predict the
LOS of a trauma patient, the severity of their injury should affect the final
LOS more than whether or not they can speak English (which could be just two
of many features that are recorded about a patient). Our contribution is thus
a modified distance function to take the relative importance of attributes into
account. 

\paragraph{Mathematical formulation}
Given two instances $x = (x_1,x_2,\ldots,x_n)$ and $y = (y_1,y_2,\ldots,y_n)$,
the distance between them is given by:
\begin{equation*}
\mathrm{Distance} = \sum_{i=1}^n w_i |x_i-y_i|
\end{equation*}
where the $w_i$s weight the contribution of the $i$-th feature to the overall
distance that is computed by the $k$-NN algorithm. We assume that all feature
values have been normalised, as described above.

\paragraph{Assignment of weights}
The weights $w_i$ can be tuned to match the particular problem that is being
investigated. For our LOS classification problem, we will consider two ways
of weighting features:
\begin{enumerate}
\item The weight of each feature is the magnitude of the correlation
coefficient
between it and the class, meaning that features that are more highly correlated
with the class will contribute more to the distance. The correlation
coefficient $r_i$ for feature $i$ with the class $c$ can be computed by:
\begin{equation*}
r_i = \dfrac{\sum_{i=1}^m (x_i-\bar{x})(c-\bar{c})}{\sqrt{(\sum_{i=1}^m x_i-\bar{x})(\sum_{i=1}^m c-\bar{c})}}
\end{equation*}
where $m$ is the number of training examples, $x_i$ is the value of feature
$i$, and $\bar{x}$ and $\bar{c}$ are the arithmetic averages of the feature
values and the class values respectively. 
\item We compute the correlation coefficients as described in the point above.
This gives us a ranking of the importance of each feature to the prediction of
the class, with 1 being the highest rank (indicating the most importance) and
$n$ being the lowest rank (indicating the least importance).
Instead of using the values of the correlation directly to weight
the distance contribution of each feature, we specify a function:
\begin{equation*}
f : \mathrm{Rank} \rightarrow \mathbb{R}, \mathrm{Rank} \in \{1,2,\ldots,n\}
\end{equation*}
that describes
how the weights of the features vary with the rank. This function should
inituitively be non-increasing and should produce lower values when the rank
number is greater, meaning that less important features have a lower weight
in the distance calculation.
Note that choosing $f(\mathrm{Rank}) = k$ for any non-zero constant $k$
results in all features contributing equally to the distance calculation.

\noindent We will use $f(\mathrm{Rank}) = \frac{1}{\mathrm{Rank}}$ and
$f(\mathrm{Rank}) = n-\mathrm{Rank}$.
\end{enumerate}

\section{Multi-layer Perceptron (MLP)}

\section{Conclusion}
