\chapter{Experimental Setup} \label{chap:experiments}

In this chapter we detail the steps carried out as part of our investigation.
We first outline the nature of the data set, describe how it was cleaned and
pre-processed before it was used to train various classifiers. The methods of
evaluation are then presented, as these are critical to assessing the
performance of the models created.

\section{Trauma patient data set}

\subsection{Collection}
The data set used consisted of trauma registry data from the trauma centre
at the Royal Prince Alfred Hospital, a major trauma centre in New South Wales,
Australia. It covered all adult (age 15 and over) inpatient admissions to the
trauma centre from 2007--2011. 

All patients were first admitted to the trauma ward until discharged
or transferred to an appropriate unit within the hospital. A single trained
data manager recorded a variety of attributes about the admitted patient,
such as age, gender, blood pressure, mechanism of injury and body regions
that were injured. We received this data in a Microsoft Excel spreadsheet.

\subsection{Characteristics}
There were 2546 patient records in the data set we received, comprising of 74
features, one of which was the target variable \textbf{LOS48}. LOS48 was a
binary variable -- that is, it could only take two values, 0 or 1 -- with 1
indicating that the patient stayed two days or less, and 0 indicating a stay
of greater than 2 days.

\todo{Perhaps insert some more information about the data itself, such
as the various attributes and their data types? Not sure if that should go
here, in the results, or in an appendix -- it is relevant but can disturb
the flow of the main text.}

\section{Data preparation}
In any data mining task, adequately preparing the data ensures the best
chance of success: this includes deciding how to deal with missing and
inaccurate values, as well as appropriately transforming the data
\citep{Witten2005} and selecting or constructing the most suitable features
for the task \citep{Kotsiantis2006}.
In this section we describe how we addressed each of
the major issues and the reasons for our choice.

\subsection{Cleaning and normalisation} % missing values, irrelevant features
Since the data was initially in an Excel spreadsheet, we explored the data
by hand using Microsoft Excel's filter function on each column and
consolidated many categorical values simply
by correcting spelling and punctuation. For example:
\begin{itemize}
\item In the \textbf{sex} attribute, there were values of `Female' and
`Femal' which were consolidated into `Female'.
\item In the \textbf{mechanism} attribute, which was textual and indicated
the category of how the patient was injured, there were such categories as
`Other Vehi' and `Other Vehicle' as well as `Pedal Cycl' and `Pedal Cyclist'.
Such categories were combined into `Other Vehicle' and `Pedal Cyclist'
respectively.
\item Some categories in \textbf{mechanism} were also divided into whether or
not the mechanism of injury was self-inflicted, such as `Shooting' and
`Shooting-selfinflicted'. When we asked the domain expert, Michael Dinh, his
suggestion was to combine them into one category: in this case, simply
`Shooting'; this was done for all the categories in this attribute.
\item \textbf{disp from ED} indicated the section of the hospital that the
patient was discharged to. This was also a textual categorical attribute.
There were redundancies like `G. ICU' and `G.ICU' and even `GICU', which we
all combined into one category, `GICU'.
\end{itemize}

The data was then exported into CSV (comma-separated value) format, so that
we could continue to process it programmatically rather than manually. Many
fields in the data set contained leading or trailing whitespace, which was
removed with a few lines of Python code (see appendix \todo{[add reference
to appendix]}). All attribute names were converted to lowercase. In the case
where attribute names consisted of several words, the spaces were replaced
with underscores (\_) to ensure that the full attribute name was read.
\todo{Include a list of these converted attribute names? In the appendix?}

In order for our results to be comparable to that of Dinh
et. al. \citep{Dinh2013a}, we removed all instances with missing values,
leaving 2517 instances with which to use to create our prediction models.

\subsection{Feature selection}
After data preparation, selecting a subset of features or attributes to use
in a model is also important, and helps to reduce noise and redundant
attributes from becoming part of the final model. Experiments have found that
the performance of various classification methods such as decision trees
\textit{deteriorates} in the presence of useless attributes.
Our data set contains 73
non-target attributes, which is a fairly high number. Many of these attributes
are derived from others in the set, such as \textbf{age} and \textbf{age65}.

We began the process of feature selection by removing attributes that were
not available upon admission, as these tended to be highly related to the
LOS48 outcome and were a consequence, not a predictor, of LOS. These attributes
were: iculos, outcome, disp from ED, disp from hosp, died, rehab and los.
The id attribute was also removed because it simply numbered each record and
indicated nothing about the patient themself. This left us with 67 attributes.

The work of actually selecting the attributes out of the 67 available was
carried out in two ways: automated using feature selection methods, and
manually using advice from a domain expert.

\subsubsection{Automated feature selection methods}
There are a number of methods that are commonly used to automate the selection
of a subset of features for the final classifier. The methods we used were:
\begin{itemize}
\item \textit{Correlation-based feature selection} \citep{Hall2000},
which evaluates the `merit' of a
subset of features by considering their degree of correlation with the target
variable compared to their degree of correlation amongst each other. High
correlation between variables indicates redundancy. Under
this scheme, a set of features that are highly correlated with the target
while having low correlation with other features in the set is ideal.
\item Selection based on the
\textit{Pearson product-moment correlation coefficient}, a measure of the
linear correlation or dependence between two variables.\todo{add some more info}
\item \textit{Information gain} \todo{explain a bit about this}
\item Feature selection based on the \textit{OneR classifier}
\citep{Holte1993}, which uses one
attribute as the predictor for the target. This attribute is typically the one
that has the lowest prediction error. Feature selection using this scheme
involves first ranking attributes based on their error, with the lowest error
attributes as the most desirable. We then selected the top 20 attributes to
use in the building of the model.
\end{itemize}

We used the WEKA \citep{Hall2009} implementation of the above methods, after
converting the data into the Attribute-Relation File Format
(ARFF)\footnote{\url{http://weka.wikispaces.com/ARFF+\%28developer+version\%29}}
most suitable for the program.
\todo{give more details, ie specifying binary/numeric
attributes, maybe specify in appendix?}

\subsubsection{Expert selection}
We also wanted to compare the feature selection of an expert in the domain
of the data set, namely trauma, with the models obtained from the features
selected using the automated methods described above. This resulted in two
feature subsets. The first was obtained simply by presenting the entire list
of 73 features to the domain expert and asking him to select the features he
thought were most indicative of the target variable from experience. The
second set was obtained from the work of Dinh et. al. \citep{Dinh2013a} that
we are building on, giving us a baseline to compare our results to.

\section{Model construction}

\subsection{Single classifier}

\subsubsection{Parameter tuning}

\subsection{Ensemble classifiers}

\section{Performance evaluation} % don't forget cross-validation

\subsection{Metrics}

\subsection{Statistical tests}
